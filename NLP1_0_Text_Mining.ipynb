{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP1-0_Text_Mining.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "oldHeight": 313.667,
      "position": {
        "height": "335.667px",
        "left": "935px",
        "right": "20px",
        "top": "173px",
        "width": "565px"
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "varInspector_section_display": "block",
      "window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ppkk0906/Caba_nlp/blob/main/NLP1_0_Text_Mining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfgFhH-17V_W"
      },
      "source": [
        "## NLP, 텍스트 분석\n",
        "- Natural Language Processing: 자연어처리\n",
        "- 텍스트 분석: 비정형 텍스트에서 의미있는 정보 추출\n",
        "- NLP와 텍스트 분석의 근간에는 머신러닝이 존재. 과거 언어적인 룰 기반 시스템에서 텍스트 데이터 기반으로 모델을 학습하고 예측\n",
        "- 텍스트 분석은 머신러닝, 언어 이해, 통계 등을 활용한 모델 수립, 정보 추출을 통해 인사이트 및 예측 분석 등의 분석 작업 수행\n",
        "  - 텍스트 분류: 신문기사 카테고리 분류, 스팸 메일 검출, 지도학습\n",
        "  - 감성 분석: 감정/판단/믿음/의견/기분 등의 주관적 요소 분석, 지도 또는 비지도 학습\n",
        "  - 텍스트 요약: 텍스트 내에서 중요한 주제나 중심 사상을 추출. 토픽 모델링\n",
        "  - 텍스트 군집화와 유사도 측정: 비슷한 유형의 문서에 대해 군집화 수행, 비지도 학습\n",
        "\n",
        "## 텍스트 분석 수행 프로세스\n",
        "  - 텍스트 정규화\n",
        "    - 클랜징, 토큰화, 필터링/스톱워드 제거/철자 수정\n",
        "  - 피처 벡터화 변환\n",
        "    - Bag of words: Count 기반, TF-IDF 기반\n",
        "    - Word2Vec\n",
        "  - ML모델 수립 및 학습/예측/평가\n",
        "## 텍스트 전처리 - 텍스트 정규화\n",
        "  - 클렌징: 분석에 방해되는 불필요한 문자, 기호를 사전에 제거 예) html,xml 태그\n",
        "  - 토큰화: 문서에 문장을 분리하는 문장 토큰화와 단어를 토큰으로 분리하는 단어 토큰화\n",
        "  - 필터링/스톱워드 제거/철자 수정: 분석에 큰 의미가 없는 단어를 제거\n",
        "  - Stemming Lemmatization: 문법적, 의미적으로 변화하는 단어의 원형을 찾음\n",
        "    - Stemming: 원형 단어로 변환 시 일반적인 방법을 적용하거나 더 단순화된 방법을 찾음\n",
        "    - Lemmatization: Stemming보다 정교하며 의미론적인 기반에서 단어의 원형을 찾음\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-09T02:48:21.441819Z",
          "start_time": "2021-03-09T02:48:20.201176Z"
        },
        "id": "c99FcqCdqJ-2"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "import nltk\n",
        "from nltk import sent_tokenize\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from konlpy.tag  import Okt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-05T02:36:18.759381Z",
          "start_time": "2021-03-05T02:36:17.559809Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMQqIpOu7GUZ",
        "outputId": "f7a90873-c6f1-40ee-a0dd-17ed481c20ad"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-05T02:36:18.775173Z",
          "start_time": "2021-03-05T02:36:18.760127Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WR7fmJvJ7VQo",
        "outputId": "2e6a3e71-88a6-4055-9b44-e8a72662161f"
      },
      "source": [
        "text_sample = 'The Matrix is everywhere ites all around us, here even in this room. You can see it out your window or on your television. You feel it when you go to work, or go to church or pay your taxes'\n",
        "sentences = sent_tokenize(text_sample)\n",
        "print(sentences)\n",
        "print(type(sentences), len(sentences))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['The Matrix is everywhere ites all around us, here even in this room.', 'You can see it out your window or on your television.', 'You feel it when you go to work, or go to church or pay your taxes']\n",
            "<class 'list'> 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-05T02:36:18.892300Z",
          "start_time": "2021-03-05T02:36:18.778166Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8N-Gzf8EQO-",
        "outputId": "01a54c2a-b2e0-402c-eab4-0181ff0c2933"
      },
      "source": [
        "# 단어 토큰화(work_tokenize): 공백, 콤마, 마침표, 개행문자, 정규표현식\n",
        "from nltk import word_tokenize\n",
        "words = word_tokenize(sentences[0])\n",
        "print(words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['The', 'Matrix', 'is', 'everywhere', 'ites', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-05T02:36:18.970312Z",
          "start_time": "2021-03-05T02:36:18.892379Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-X_IdfSFGxa",
        "outputId": "3323375f-7472-42a5-8700-d6b525e1323a"
      },
      "source": [
        "def get_words(text):\n",
        "  sentences = sent_tokenize(text)\n",
        "  words=[]\n",
        "  for s in sentences:\n",
        "    for i in word_tokenize(s):\n",
        "      words.append(i.lower())\n",
        "  return words\n",
        "\n",
        "print(get_words(text_sample))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['the', 'matrix', 'is', 'everywhere', 'ites', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.', 'you', 'can', 'see', 'it', 'out', 'your', 'window', 'or', 'on', 'your', 'television', '.', 'you', 'feel', 'it', 'when', 'you', 'go', 'to', 'work', ',', 'or', 'go', 'to', 'church', 'or', 'pay', 'your', 'taxes']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-05T02:36:19.086409Z",
          "start_time": "2021-03-05T02:36:18.974739Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "866sGutRHmCm",
        "outputId": "96f799d7-8f9e-4514-a6bb-fc30200481bf"
      },
      "source": [
        "# 스톱워드 제거: the, a, is, in, will과 같이 문맥적으로 큰 의미가 없는 단어 제거\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-05T02:36:19.181810Z",
          "start_time": "2021-03-05T02:36:19.086409Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPPU1Q7MHBCo",
        "outputId": "0a765a1b-ee99-4b6e-e1c8-7a900cefb958"
      },
      "source": [
        "# NLTK english stopwords 확인\n",
        "print(len(nltk.corpus.stopwords.words('english')))\n",
        "print(nltk.corpus.stopwords.words('english'[:20]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "179\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-05T02:36:19.277570Z",
          "start_time": "2021-03-05T02:36:19.181810Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgOZWhrRMzeP",
        "outputId": "a1c754f6-878f-4156-f2d5-891e123c8b13"
      },
      "source": [
        "# 문법적 또는 의미적으로 변화하는 단어의 원형을 찾는 방법\n",
        "# Stemmer(LancasterStemmer)\n",
        "from nltk.stem import LancasterStemmer\n",
        "stemmer = LancasterStemmer()\n",
        "print(stemmer.stem('working'),stemmer.stem('works'),stemmer.stem('worked'))\n",
        "print(stemmer.stem('amusing'),stemmer.stem('aumses'),stemmer.stem('amused'))\n",
        "print(stemmer.stem('fancier'),stemmer.stem('fancist'))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "work work work\n",
            "amus aums amus\n",
            "fant fant\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-05T02:36:19.403697Z",
          "start_time": "2021-03-05T02:36:19.277731Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfofLufglcZP",
        "outputId": "86242563-5677-4518-fbd9-61cee116b006"
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-05T02:36:20.551892Z",
          "start_time": "2021-03-05T02:36:19.403697Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJe5XMgwqKHJ",
        "outputId": "ed611b4e-2638-423a-d55a-61288499f965"
      },
      "source": [
        "# Lemmatization(WordNetLemmatizer) : 정확한 원형 단어 추출을 위해 단어의 품사를 직접 입력\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "lemma = WordNetLemmatizer()\n",
        "print(lemma.lemmatize('working','v'),lemma.lemmatize('works','v'),lemma.lemmatize('worked','v'))\n",
        "print(lemma.lemmatize('amusing','v'),lemma.lemmatize('aumses','v'),lemma.lemmatize('amused','v'))\n",
        "print(lemma.lemmatize('fancier','a'),lemma.lemmatize('fancier','a'))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "work work work\n",
            "amuse aumses amuse\n",
            "fancy fancy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6BAW0LDsFlR"
      },
      "source": [
        "1. 피처 벡터화 : One-hot encoding\n",
        "2. Bag of Words : 문맥이나 순서를 무시하고 일괄적으로 단어에 대한 빈도 값을 부여해 피처 값을 추출하는 모델.\n",
        "  - 단점 : 문맥 의미 반영 부족, 희소 행렬 문제.\n",
        "  - BOW에서 피처 벡터화 : 모든 단어를 컬럼 형태로 나열하고 각 문서에서 해당 단어의 횟수나 정규화된 빈도를 값으로 부여하는 데이터 세트 모델로 변경하는 것.\n",
        "\n",
        "3. 피처 벡터화 방식 : 카운트 기반, TF-IDF(Term Frequency - Inverse Document Frequency) 기반 벡터화\n",
        "4. 카운트 벡터화 : 카운트 값이 높을수록 중요한 단어로 인식. 특성상 자주 사용되는 보편적인 단어까지 높은 값 부여\n",
        "5. TF-IDF : 모든 문서에서 전반적으로 자주 나타나는 단어에 대해서 패널티 부여. '빈번하게', '당연히', '조직', '업무' 등  \n",
        "  \n",
        "  파라미터\n",
        "- max_df : 너무 높은 빈도수를 가지는 단어 피처를 제외\n",
        "- min_df : 너무 낮은 빈도수를 가지는 단어 피처를 제외\n",
        "- max_features : 추출하는 피처의 개수를 제한하며 정수로 값을 지정\n",
        "- stop_words : 'english'로 지정하면 스톱 워드로 지정된 단어는 추출에서 제외\n",
        "- n_gram_range : 튜플 형태로 (범위 최솟값, 범위 최댓값)을 지정\n",
        "- analyzer : 피처 추출을 수행하는 단위. 디폴트는 'word'\n",
        "- token_pattern : 토큰화를 수행하는 정규 표현식 패턴을 지정\n",
        "- tokenizer : 토큰화를 별도의 커스텀 함수로 이용시 적용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-05T02:36:20.567932Z",
          "start_time": "2021-03-05T02:36:20.553059Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCAg4BIvyU7g",
        "outputId": "f77c5623-b3a4-48a3-e65c-b2ff75b1d139"
      },
      "source": [
        "#ndarray 객체 생성\n",
        "dense = np.array([[3,0,1], [0,2,0]])\n",
        "dense"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3, 0, 1],\n",
              "       [0, 2, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-05T02:36:20.665773Z",
          "start_time": "2021-03-05T02:36:20.568377Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9zgxT-ayhjU",
        "outputId": "68f343b2-7aba-4639-bd81-291f5398b43f"
      },
      "source": [
        "# 희소행렬 - coo형식\n",
        "from scipy import sparse\n",
        "data = np.array([3,1,2])\n",
        "row_pos = np.array([0,0,1])\n",
        "col_pos = np.array([0,2,1])\n",
        "sparse_coo = sparse.coo_matrix((data,(row_pos, col_pos)))\n",
        "print(sparse_coo)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (0, 0)\t3\n",
            "  (0, 2)\t1\n",
            "  (1, 1)\t2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-05T02:36:20.774902Z",
          "start_time": "2021-03-05T02:36:20.665928Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVX0gkFRzdmV",
        "outputId": "df25e80e-6c5c-4e36-8a85-d650849e7d1a"
      },
      "source": [
        "# 희소 행렬 - COO 형식\n",
        "dense2 = np.array([[0,0,1,0,0,5],\n",
        "                   [1,4,0,3,2,5],\n",
        "                   [0,6,0,3,0,0],\n",
        "                   [2,0,0,0,0,0],\n",
        "                   [0,0,0,7,0,8],\n",
        "                   [1,0,0,0,0,0]])\n",
        "data2 = np.array([1,5,1,4,3,2,5,6,3,2,7,8,1])\n",
        "row_pos2=np.array([0,0,1,1,1,1,1,2,2,3,4,4,5])\n",
        "col_pos2=np.array([2,5,0,1,3,4,5,1,3,0,3,5,0])\n",
        "print(sparse.coo_matrix((data2,(row_pos2, col_pos2))).toarray(), '\\n')\n",
        "#희소 행렬 - CSR 형식\n",
        "row_pos_cul = ([0,2,7,9,10,12,13])\n",
        "print(sparse.csr_matrix((data2,col_pos2,row_pos_cul)).toarray())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 0 1 0 0 5]\n",
            " [1 4 0 3 2 5]\n",
            " [0 6 0 3 0 0]\n",
            " [2 0 0 0 0 0]\n",
            " [0 0 0 7 0 8]\n",
            " [1 0 0 0 0 0]] \n",
            "\n",
            "[[0 0 1 0 0 5]\n",
            " [1 4 0 3 2 5]\n",
            " [0 6 0 3 0 0]\n",
            " [2 0 0 0 0 0]\n",
            " [0 0 0 7 0 8]\n",
            " [1 0 0 0 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-05T02:36:20.858743Z",
          "start_time": "2021-03-05T02:36:20.774902Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prcK6jQ33NCk",
        "outputId": "713e078b-a974-4ad4-a569-fe177ca1d04a"
      },
      "source": [
        "# DictVectorizer : 문서에서 단어의 사용빈도를 나타내는 딕셔너리 정보를 입력받아 BOW 인코딩한 수치 벡터로 전환\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "v = DictVectorizer(sparse=False)\n",
        "D = [{'A':1, 'B':2}, {'B':2, 'C':1}]\n",
        "X = v.fit_transform(D)\n",
        "print(X)\n",
        "print(v.feature_names_)\n",
        "print(v.vocabulary_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1. 2. 0.]\n",
            " [0. 2. 1.]]\n",
            "['A', 'B', 'C']\n",
            "{'A': 0, 'B': 1, 'C': 2}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-05T02:36:20.939301Z",
          "start_time": "2021-03-05T02:36:20.859827Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HV7yGz2W6zad",
        "outputId": "fd3fd476-b96e-4dcd-cf96-6897b3f1b109"
      },
      "source": [
        "# CountVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "corpus = [\n",
        "          'This is the first document',\n",
        "          'This is the second document.',\n",
        "          'And the third one...',\n",
        "          'Is this the first document?',\n",
        "          'The last document???'\n",
        "]\n",
        "c = CountVectorizer()\n",
        "c.fit(corpus)\n",
        "print(c.get_feature_names())\n",
        "print(c.transform(corpus))\n",
        "print(c.vocabulary_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['and', 'document', 'first', 'is', 'last', 'one', 'second', 'the', 'third', 'this']\n",
            "  (0, 1)\t1\n",
            "  (0, 2)\t1\n",
            "  (0, 3)\t1\n",
            "  (0, 7)\t1\n",
            "  (0, 9)\t1\n",
            "  (1, 1)\t1\n",
            "  (1, 3)\t1\n",
            "  (1, 6)\t1\n",
            "  (1, 7)\t1\n",
            "  (1, 9)\t1\n",
            "  (2, 0)\t1\n",
            "  (2, 5)\t1\n",
            "  (2, 7)\t1\n",
            "  (2, 8)\t1\n",
            "  (3, 1)\t1\n",
            "  (3, 2)\t1\n",
            "  (3, 3)\t1\n",
            "  (3, 7)\t1\n",
            "  (3, 9)\t1\n",
            "  (4, 1)\t1\n",
            "  (4, 4)\t1\n",
            "  (4, 7)\t1\n",
            "{'this': 9, 'is': 3, 'the': 7, 'first': 2, 'document': 1, 'second': 6, 'and': 0, 'third': 8, 'one': 5, 'last': 4}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-05T02:36:21.024920Z",
          "start_time": "2021-03-05T02:36:20.940299Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCEOxWotAQJn",
        "outputId": "fe5cc077-2d71-44e6-f840-4bdbf100a46d"
      },
      "source": [
        "print(c.transform(['this is the second document']).toarray())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 1 0 1 0 0 1 1 0 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-05T02:36:21.123162Z",
          "start_time": "2021-03-05T02:36:21.029018Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6rpz4YuBNd-",
        "outputId": "5782a142-52aa-4188-da68-3c9486af9de7"
      },
      "source": [
        "# Stop Words는 문서에서 단어장을 생성할 때 무시할 수 있는 단어. \n",
        "# 보통 영어의 관사, 접속사, 한국어의 조사 등\n",
        "\n",
        "vect = CountVectorizer(stop_words=['and','is','the','this']).fit(corpus)\n",
        "vect.vocabulary_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'first': 1, 'document': 0, 'second': 4, 'third': 5, 'one': 3, 'last': 2}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-05T02:36:21.210833Z",
          "start_time": "2021-03-05T02:36:21.129148Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76FPOk5qB9Gf",
        "outputId": "ac4d17c3-faaa-4fae-f0ef-2cc66c26a517"
      },
      "source": [
        "vect = CountVectorizer(stop_words='english').fit(corpus)\n",
        "vect.vocabulary_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'document': 0, 'second': 1}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-05T02:36:21.319623Z",
          "start_time": "2021-03-05T02:36:21.213821Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSHjS2RTC6HX",
        "outputId": "9e250e70-e54f-48a7-f861-68effee85e86"
      },
      "source": [
        "# analyzer, tokenizer, token_pattern 등의 인수로 사용할 토큰 생성기를 선택\n",
        "vect = CountVectorizer(analyzer=\"char\").fit(corpus)\n",
        "vect.vocabulary_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'t': 16,\n",
              " 'h': 8,\n",
              " 'i': 9,\n",
              " 's': 15,\n",
              " ' ': 0,\n",
              " 'e': 6,\n",
              " 'f': 7,\n",
              " 'r': 14,\n",
              " 'd': 5,\n",
              " 'o': 13,\n",
              " 'c': 4,\n",
              " 'u': 17,\n",
              " 'm': 11,\n",
              " 'n': 12,\n",
              " '.': 1,\n",
              " 'a': 3,\n",
              " '?': 2,\n",
              " 'l': 10}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-05T02:36:21.429496Z",
          "start_time": "2021-03-05T02:36:21.319623Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ww0ft3OfDuHI",
        "outputId": "37b3632b-a045-4c62-e896-bf0598532301"
      },
      "source": [
        "# n-그램은 단어장 생성에 사용할 토큰의 크기 결정\n",
        "vect = CountVectorizer(ngram_range=(1,2)).fit(corpus)\n",
        "print(vect.vocabulary_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'this': 20, 'is': 5, 'the': 13, 'first': 3, 'document': 2, 'this is': 21, 'is the': 6, 'the first': 14, 'first document': 4, 'second': 11, 'the second': 16, 'second document': 12, 'and': 0, 'third': 18, 'one': 10, 'and the': 1, 'the third': 17, 'third one': 19, 'is this': 7, 'this the': 22, 'last': 8, 'the last': 15, 'last document': 9}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-05T02:36:21.482719Z",
          "start_time": "2021-03-05T02:36:21.429496Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLbr3HCwB_4l",
        "outputId": "3da7ccfc-19fe-4003-dff1-52f76faee6c1"
      },
      "source": [
        "vect = CountVectorizer(token_pattern='t\\w+').fit(corpus)\n",
        "vect.vocabulary_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'this': 2, 'the': 0, 'third': 1}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-05T02:36:21.573850Z",
          "start_time": "2021-03-05T02:36:21.490699Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFb-zfU2CC_D",
        "outputId": "9ccb11d9-5f95-471b-f0d3-8f9e4ff6abb0"
      },
      "source": [
        "# n-그램은 단어장 생성에 사용할 토큰의 크기 결정\n",
        "vect = CountVectorizer(ngram_range=(1,2)).fit(corpus)\n",
        "vect.vocabulary_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'this': 20,\n",
              " 'is': 5,\n",
              " 'the': 13,\n",
              " 'first': 3,\n",
              " 'document': 2,\n",
              " 'this is': 21,\n",
              " 'is the': 6,\n",
              " 'the first': 14,\n",
              " 'first document': 4,\n",
              " 'second': 11,\n",
              " 'the second': 16,\n",
              " 'second document': 12,\n",
              " 'and': 0,\n",
              " 'third': 18,\n",
              " 'one': 10,\n",
              " 'and the': 1,\n",
              " 'the third': 17,\n",
              " 'third one': 19,\n",
              " 'is this': 7,\n",
              " 'this the': 22,\n",
              " 'last': 8,\n",
              " 'the last': 15,\n",
              " 'last document': 9}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tT_8uQZ2E2gQ"
      },
      "source": [
        "TF-IDF(Term Frequency – Inverse Document Frequency) 인코딩은 단어를 갯수 그대로 카운트하지 않고 모든 문서에 공통적으로 들어있는 단어의 경우 문서 구별 능력이 떨어진다고 보아 가중치를 축소\n",
        "문서 d(document)와 단어 t 에 대해 다음과 같이 계산\n",
        "\n",
        "- tf-idf(d,t)=tf(d,t)⋅idf(t)\n",
        "\n",
        "- tf(d,t): term frequency. 특정한 단어의 빈도수\n",
        "- idf(t) : inverse document frequency. 특정한 단어가 들어 있는 문서의 수에 반비례하는 수\n",
        "- n : 전체 문서의 수\n",
        "- df(t) : 단어 t 를 가진 문서의 수\n",
        "- idf(d,t)=log(n/(1+df(t))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-05T02:36:21.664648Z",
          "start_time": "2021-03-05T02:36:21.575351Z"
        },
        "id": "LOydLWmOE15H"
      },
      "source": [
        "corpus = [\"\"\"\n",
        "The Corpus of Contemporary American English (COCA) is the only large, genre-balanced corpus of American English.\n",
        "COCA is probably the most widely-used corpus of English, and it is related to many other corpora of English that we have created, which offer unparalleled insight into variation in English.\n",
        "The corpus contains more than one billion words of text (25+ million words each year 1990-2019) from eight genres: spoken, fiction, popular magazines, newspapers, academic texts, and (with the update in March 2020): TV and Movies subtitles, blogs, and other web pages.\n",
        "Click on any of the links in the search form to the left for context-sensitive help, and to see the range of queries that the corpus offers.\n",
        "\"\"\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-05T02:36:21.780638Z",
          "start_time": "2021-03-05T02:36:21.664648Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ik6uUlhbGsn6",
        "outputId": "05c53f85-8650-4f19-ddc2-9b0843f8de86"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidv = TfidfVectorizer(stop_words = 'english').fit(corpus)\n",
        "print(tfidv.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['1990', '2019', '2020', '25', 'academic', 'american', 'balanced', 'billion', 'blogs', 'click', 'coca', 'contains', 'contemporary', 'context', 'corpora', 'corpus', 'created', 'english', 'fiction', 'form', 'genre', 'genres', 'help', 'insight', 'large', 'left', 'links', 'magazines', 'march', 'million', 'movies', 'newspapers', 'offer', 'offers', 'pages', 'popular', 'probably', 'queries', 'range', 'related', 'search', 'sensitive', 'spoken', 'subtitles', 'text', 'texts', 'tv', 'unparalleled', 'update', 'used', 'variation', 'web', 'widely', 'words', 'year']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gpu6nHsD1veL"
      },
      "source": [
        "[KoNLP 패키지 사용 방법](https://mr-doosun.tistory.com/22)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-05T02:36:26.123571Z",
          "start_time": "2021-03-05T02:36:21.780638Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ksQnfNNcv2qF",
        "outputId": "baadb591-f3da-4888-fbd0-f114a156c0b3"
      },
      "source": [
        "import jpype\n",
        "from konlpy.tag import Okt, Hannanum, Kkma, Komoran, Mecab\n",
        "okt = Okt()\n",
        "print(okt.morphs('오늘은 피자 먹는 날'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['오늘', '은', '피자', '먹는', '날']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-05T02:36:26.163186Z",
          "start_time": "2021-03-05T02:36:26.123571Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSkTQjjH56Ht",
        "outputId": "92b86b55-d914-4aaf-8bd2-44c27ffcdaa1"
      },
      "source": [
        "print(okt.nouns('유일하게 항공기 체계 종합개발 경험을 갖고 있는 KAI는'))\n",
        "print(okt.nouns('나는 잠을 자고 싶다'))\n",
        "print(okt.pos('아름다운 꽃과 파란 하늘', ))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['항공기', '체계', '종합', '개발', '경험']\n",
            "['나', '잠', '자고']\n",
            "[('아름다운', 'Adjective'), ('꽃', 'Noun'), ('과', 'Josa'), ('파란', 'Noun'), ('하늘', 'Noun')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-05T02:36:26.341091Z",
          "start_time": "2021-03-05T02:36:26.163186Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQK3FoSO9Dic",
        "outputId": "ebb6390a-f9eb-4ab4-dbf3-a90963667bf5"
      },
      "source": [
        "for words, pos in okt.pos('아름다운 꽃과 파란 하늘'):\n",
        "  if pos == 'Adjective':\n",
        "    print(words)\n",
        "print()\n",
        "print(okt.nouns('나는 오늘 방콕에 갔다'))\n",
        "print()\n",
        "for words, pos in okt.pos('나는 오늘 방콕에 갔다.', stem=True):\n",
        "    print(words)\n",
        "print()\n",
        "print(okt.morphs('친절한 코치와 재미있는 친구들이 있는 도장에 가고 싶다.', ))\n",
        "print()\n",
        "print(okt.pos('나는 오늘도 장에 가고 싶다.', join=True))\n",
        "print()\n",
        "print(okt.pos('나는 오늘도 장에 가고 싶을깤ㅋㅋ?.', norm=True, stem=True))\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "아름다운\n",
            "\n",
            "['나', '오늘', '방콕']\n",
            "\n",
            "나\n",
            "는\n",
            "오늘\n",
            "방콕\n",
            "에\n",
            "가다\n",
            ".\n",
            "\n",
            "['친절한', '코치', '와', '재미있는', '친구', '들', '이', '있는', '도장', '에', '가고', '싶다', '.']\n",
            "\n",
            "['나/Noun', '는/Josa', '오늘/Noun', '도/Josa', '장/Noun', '에/Josa', '가고/Verb', '싶다/Verb', './Punctuation']\n",
            "\n",
            "[('나', 'Noun'), ('는', 'Josa'), ('오늘', 'Noun'), ('도', 'Josa'), ('장', 'Noun'), ('에', 'Josa'), ('가다', 'Verb'), ('싶다', 'Verb'), ('ㅋㅋ', 'KoreanParticle'), ('?.', 'Punctuation')]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0SPj2N9CVhp"
      },
      "source": [
        "## 텍스트 분류\n",
        "- 특정 문서의 분류를 학습 데이터를 통해 학습해 모델을 생성한 뒤 이 학습 모델을 이용해 다른 문서의 분류를 예측\n",
        "- 텍스트를 피처 벡터화로 변환, 희소 행렬로 만들고 로지스틱 회귀를 이용해 분류 수행\n",
        "- Count기반과 TF-IDF기반의 벡터화를 각각 적용, 성능 비교\n",
        "- 피처 벡터화를 위한 GridSearchCV기반의 하이퍼파라미터 튜닝을 일괄적으로 수행"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-09T00:37:00.062842Z",
          "start_time": "2021-03-09T00:31:52.395Z"
        },
        "id": "70xrb9t8AO9p"
      },
      "source": [
        "news_data = fetch_20newsgroups(subset='all', random_state=0)\n",
        "news_data.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-05T02:36:26.976787Z",
          "start_time": "2021-03-05T02:36:26.548883Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iO1GKcgNDlru",
        "outputId": "35a5ab1a-f2ca-4e8a-cc84-8b8ddd669700"
      },
      "source": [
        "print(news_data.DESCR)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".. _20newsgroups_dataset:\n",
            "\n",
            "The 20 newsgroups text dataset\n",
            "------------------------------\n",
            "\n",
            "The 20 newsgroups dataset comprises around 18000 newsgroups posts on\n",
            "20 topics split in two subsets: one for training (or development)\n",
            "and the other one for testing (or for performance evaluation). The split\n",
            "between the train and test set is based upon a messages posted before\n",
            "and after a specific date.\n",
            "\n",
            "This module contains two loaders. The first one,\n",
            ":func:`sklearn.datasets.fetch_20newsgroups`,\n",
            "returns a list of the raw texts that can be fed to text feature\n",
            "extractors such as :class:`sklearn.feature_extraction.text.CountVectorizer`\n",
            "with custom parameters so as to extract feature vectors.\n",
            "The second one, :func:`sklearn.datasets.fetch_20newsgroups_vectorized`,\n",
            "returns ready-to-use features, i.e., it is not necessary to use a feature\n",
            "extractor.\n",
            "\n",
            "**Data Set Characteristics:**\n",
            "\n",
            "    =================   ==========\n",
            "    Classes                     20\n",
            "    Samples total            18846\n",
            "    Dimensionality               1\n",
            "    Features                  text\n",
            "    =================   ==========\n",
            "\n",
            "Usage\n",
            "~~~~~\n",
            "\n",
            "The :func:`sklearn.datasets.fetch_20newsgroups` function is a data\n",
            "fetching / caching functions that downloads the data archive from\n",
            "the original `20 newsgroups website`_, extracts the archive contents\n",
            "in the ``~/scikit_learn_data/20news_home`` folder and calls the\n",
            ":func:`sklearn.datasets.load_files` on either the training or\n",
            "testing set folder, or both of them::\n",
            "\n",
            "  >>> from sklearn.datasets import fetch_20newsgroups\n",
            "  >>> newsgroups_train = fetch_20newsgroups(subset='train')\n",
            "\n",
            "  >>> from pprint import pprint\n",
            "  >>> pprint(list(newsgroups_train.target_names))\n",
            "  ['alt.atheism',\n",
            "   'comp.graphics',\n",
            "   'comp.os.ms-windows.misc',\n",
            "   'comp.sys.ibm.pc.hardware',\n",
            "   'comp.sys.mac.hardware',\n",
            "   'comp.windows.x',\n",
            "   'misc.forsale',\n",
            "   'rec.autos',\n",
            "   'rec.motorcycles',\n",
            "   'rec.sport.baseball',\n",
            "   'rec.sport.hockey',\n",
            "   'sci.crypt',\n",
            "   'sci.electronics',\n",
            "   'sci.med',\n",
            "   'sci.space',\n",
            "   'soc.religion.christian',\n",
            "   'talk.politics.guns',\n",
            "   'talk.politics.mideast',\n",
            "   'talk.politics.misc',\n",
            "   'talk.religion.misc']\n",
            "\n",
            "The real data lies in the ``filenames`` and ``target`` attributes. The target\n",
            "attribute is the integer index of the category::\n",
            "\n",
            "  >>> newsgroups_train.filenames.shape\n",
            "  (11314,)\n",
            "  >>> newsgroups_train.target.shape\n",
            "  (11314,)\n",
            "  >>> newsgroups_train.target[:10]\n",
            "  array([ 7,  4,  4,  1, 14, 16, 13,  3,  2,  4])\n",
            "\n",
            "It is possible to load only a sub-selection of the categories by passing the\n",
            "list of the categories to load to the\n",
            ":func:`sklearn.datasets.fetch_20newsgroups` function::\n",
            "\n",
            "  >>> cats = ['alt.atheism', 'sci.space']\n",
            "  >>> newsgroups_train = fetch_20newsgroups(subset='train', categories=cats)\n",
            "\n",
            "  >>> list(newsgroups_train.target_names)\n",
            "  ['alt.atheism', 'sci.space']\n",
            "  >>> newsgroups_train.filenames.shape\n",
            "  (1073,)\n",
            "  >>> newsgroups_train.target.shape\n",
            "  (1073,)\n",
            "  >>> newsgroups_train.target[:10]\n",
            "  array([0, 1, 1, 1, 0, 1, 1, 0, 0, 0])\n",
            "\n",
            "Converting text to vectors\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "In order to feed predictive or clustering models with the text data,\n",
            "one first need to turn the text into vectors of numerical values suitable\n",
            "for statistical analysis. This can be achieved with the utilities of the\n",
            "``sklearn.feature_extraction.text`` as demonstrated in the following\n",
            "example that extract `TF-IDF`_ vectors of unigram tokens\n",
            "from a subset of 20news::\n",
            "\n",
            "  >>> from sklearn.feature_extraction.text import TfidfVectorizer\n",
            "  >>> categories = ['alt.atheism', 'talk.religion.misc',\n",
            "  ...               'comp.graphics', 'sci.space']\n",
            "  >>> newsgroups_train = fetch_20newsgroups(subset='train',\n",
            "  ...                                       categories=categories)\n",
            "  >>> vectorizer = TfidfVectorizer()\n",
            "  >>> vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
            "  >>> vectors.shape\n",
            "  (2034, 34118)\n",
            "\n",
            "The extracted TF-IDF vectors are very sparse, with an average of 159 non-zero\n",
            "components by sample in a more than 30000-dimensional space\n",
            "(less than .5% non-zero features)::\n",
            "\n",
            "  >>> vectors.nnz / float(vectors.shape[0])\n",
            "  159.01327...\n",
            "\n",
            ":func:`sklearn.datasets.fetch_20newsgroups_vectorized` is a function which \n",
            "returns ready-to-use token counts features instead of file names.\n",
            "\n",
            ".. _`20 newsgroups website`: http://people.csail.mit.edu/jrennie/20Newsgroups/\n",
            ".. _`TF-IDF`: https://en.wikipedia.org/wiki/Tf-idf\n",
            "\n",
            "\n",
            "Filtering text for more realistic training\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "It is easy for a classifier to overfit on particular things that appear in the\n",
            "20 Newsgroups data, such as newsgroup headers. Many classifiers achieve very\n",
            "high F-scores, but their results would not generalize to other documents that\n",
            "aren't from this window of time.\n",
            "\n",
            "For example, let's look at the results of a multinomial Naive Bayes classifier,\n",
            "which is fast to train and achieves a decent F-score::\n",
            "\n",
            "  >>> from sklearn.naive_bayes import MultinomialNB\n",
            "  >>> from sklearn import metrics\n",
            "  >>> newsgroups_test = fetch_20newsgroups(subset='test',\n",
            "  ...                                      categories=categories)\n",
            "  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n",
            "  >>> clf = MultinomialNB(alpha=.01)\n",
            "  >>> clf.fit(vectors, newsgroups_train.target)\n",
            "  MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
            "\n",
            "  >>> pred = clf.predict(vectors_test)\n",
            "  >>> metrics.f1_score(newsgroups_test.target, pred, average='macro')\n",
            "  0.88213...\n",
            "\n",
            "(The example :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py` shuffles\n",
            "the training and test data, instead of segmenting by time, and in that case\n",
            "multinomial Naive Bayes gets a much higher F-score of 0.88. Are you suspicious\n",
            "yet of what's going on inside this classifier?)\n",
            "\n",
            "Let's take a look at what the most informative features are:\n",
            "\n",
            "  >>> import numpy as np\n",
            "  >>> def show_top10(classifier, vectorizer, categories):\n",
            "  ...     feature_names = np.asarray(vectorizer.get_feature_names())\n",
            "  ...     for i, category in enumerate(categories):\n",
            "  ...         top10 = np.argsort(classifier.coef_[i])[-10:]\n",
            "  ...         print(\"%s: %s\" % (category, \" \".join(feature_names[top10])))\n",
            "  ...\n",
            "  >>> show_top10(clf, vectorizer, newsgroups_train.target_names)\n",
            "  alt.atheism: edu it and in you that is of to the\n",
            "  comp.graphics: edu in graphics it is for and of to the\n",
            "  sci.space: edu it that is in and space to of the\n",
            "  talk.religion.misc: not it you in is that and to of the\n",
            "\n",
            "\n",
            "You can now see many things that these features have overfit to:\n",
            "\n",
            "- Almost every group is distinguished by whether headers such as\n",
            "  ``NNTP-Posting-Host:`` and ``Distribution:`` appear more or less often.\n",
            "- Another significant feature involves whether the sender is affiliated with\n",
            "  a university, as indicated either by their headers or their signature.\n",
            "- The word \"article\" is a significant feature, based on how often people quote\n",
            "  previous posts like this: \"In article [article ID], [name] <[e-mail address]>\n",
            "  wrote:\"\n",
            "- Other features match the names and e-mail addresses of particular people who\n",
            "  were posting at the time.\n",
            "\n",
            "With such an abundance of clues that distinguish newsgroups, the classifiers\n",
            "barely have to identify topics from text at all, and they all perform at the\n",
            "same high level.\n",
            "\n",
            "For this reason, the functions that load 20 Newsgroups data provide a\n",
            "parameter called **remove**, telling it what kinds of information to strip out\n",
            "of each file. **remove** should be a tuple containing any subset of\n",
            "``('headers', 'footers', 'quotes')``, telling it to remove headers, signature\n",
            "blocks, and quotation blocks respectively.\n",
            "\n",
            "  >>> newsgroups_test = fetch_20newsgroups(subset='test',\n",
            "  ...                                      remove=('headers', 'footers', 'quotes'),\n",
            "  ...                                      categories=categories)\n",
            "  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n",
            "  >>> pred = clf.predict(vectors_test)\n",
            "  >>> metrics.f1_score(pred, newsgroups_test.target, average='macro')\n",
            "  0.77310...\n",
            "\n",
            "This classifier lost over a lot of its F-score, just because we removed\n",
            "metadata that has little to do with topic classification.\n",
            "It loses even more if we also strip this metadata from the training data:\n",
            "\n",
            "  >>> newsgroups_train = fetch_20newsgroups(subset='train',\n",
            "  ...                                       remove=('headers', 'footers', 'quotes'),\n",
            "  ...                                       categories=categories)\n",
            "  >>> vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
            "  >>> clf = MultinomialNB(alpha=.01)\n",
            "  >>> clf.fit(vectors, newsgroups_train.target)\n",
            "  MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
            "\n",
            "  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n",
            "  >>> pred = clf.predict(vectors_test)\n",
            "  >>> metrics.f1_score(newsgroups_test.target, pred, average='macro')\n",
            "  0.76995...\n",
            "\n",
            "Some other classifiers cope better with this harder version of the task. Try\n",
            "running :ref:`sphx_glr_auto_examples_model_selection_grid_search_text_feature_extraction.py` with and without\n",
            "the ``--filter`` option to compare the results.\n",
            "\n",
            ".. topic:: Recommendation\n",
            "\n",
            "  When evaluating text classifiers on the 20 Newsgroups data, you\n",
            "  should strip newsgroup-related metadata. In scikit-learn, you can do this by\n",
            "  setting ``remove=('headers', 'footers', 'quotes')``. The F-score will be\n",
            "  lower because it is more realistic.\n",
            "\n",
            ".. topic:: Examples\n",
            "\n",
            "   * :ref:`sphx_glr_auto_examples_model_selection_grid_search_text_feature_extraction.py`\n",
            "\n",
            "   * :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py`\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-05T02:36:27.001807Z",
          "start_time": "2021-03-05T02:36:26.976787Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Z22yj1KEHcJ",
        "outputId": "ef12db73-0062-42a8-98b8-6bf647c8037d"
      },
      "source": [
        "pd.Series(news_data.target_names).value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "talk.religion.misc          1\n",
              "comp.sys.ibm.pc.hardware    1\n",
              "alt.atheism                 1\n",
              "sci.space                   1\n",
              "sci.electronics             1\n",
              "sci.crypt                   1\n",
              "talk.politics.mideast       1\n",
              "misc.forsale                1\n",
              "talk.politics.guns          1\n",
              "comp.windows.x              1\n",
              "comp.os.ms-windows.misc     1\n",
              "talk.politics.misc          1\n",
              "rec.sport.hockey            1\n",
              "rec.motorcycles             1\n",
              "soc.religion.christian      1\n",
              "sci.med                     1\n",
              "comp.sys.mac.hardware       1\n",
              "rec.autos                   1\n",
              "comp.graphics               1\n",
              "rec.sport.baseball          1\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-09T00:37:34.250086Z",
          "start_time": "2021-03-09T00:37:31.566332Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ieIZF4ntEc5E",
        "outputId": "708bfec9-f4e4-4ad6-c27b-5b840bb9cc99"
      },
      "source": [
        "# 텍스트 정규화\n",
        "# 뉴스그룹 기사내용을 제외하고 다른 정보 제거\n",
        "# 제목, 소속, 이메일 등 헤더와 푸터 정보들은 분류의 타겟 클래스 값과 유사할 수 있음\n",
        "train_news = fetch_20newsgroups(subset='train', remove=('header', 'footer', 'quotes'), random_state=0)\n",
        "X_train = train_news.data\n",
        "y_train = train_news.target\n",
        "test_news = fetch_20newsgroups(subset='test', remove=('header', 'footer', 'quotes'), random_state=0)\n",
        "X_test = test_news.data\n",
        "y_test = test_news.target\n",
        "print(len(X_train), len(X_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11314 7532\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-05T02:36:33.303251Z",
          "start_time": "2021-03-05T02:36:29.100094Z"
        },
        "id": "oTTarfa3J07I"
      },
      "source": [
        "cnt_vect = CountVectorizer()\n",
        "cnt_vect.fit(X_train)\n",
        "X_train_cnt_vect = cnt_vect.transform(X_train)\n",
        "X_test_cnt_vect = cnt_vect.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-05T02:37:14.220709Z",
          "start_time": "2021-03-05T02:36:33.303251Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ttmz3HWrK8Re",
        "outputId": "c4b9172c-c7d5-485e-8053-2243ba10001b"
      },
      "source": [
        "lr_clf = LogisticRegression()\n",
        "lr_clf.fit(X_train_cnt_vect, y_train)\n",
        "pred = lr_clf.predict(X_test_cnt_vect)\n",
        "print(accuracy_score(y_test, pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7523898035050451\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "C:\\Users\\admin\\anaconda3\\envs\\ml_python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-05T02:40:33.772349Z",
          "start_time": "2021-03-05T02:40:29.409025Z"
        },
        "id": "E6gzVA0qqJ_I"
      },
      "source": [
        "# 피처 벡터화 변환: TF-IDF 벡터화\n",
        "tfidf_vect = TfidfVectorizer()\n",
        "tfidf_vect.fit(X_train)\n",
        "X_train_tfidf_vect = tfidf_vect.transform(X_train)\n",
        "X_test_tfidf_vect = tfidf_vect.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-05T02:41:10.875792Z",
          "start_time": "2021-03-05T02:40:38.277338Z"
        },
        "id": "FWkz59fBqJ_I",
        "outputId": "9aab32f6-20e4-4b90-dc77-154dfb7130f5"
      },
      "source": [
        "lr_clf = LogisticRegression()\n",
        "lr_clf.fit(X_train_tfidf_vect, y_train)\n",
        "pred = lr_clf.predict(X_test_tfidf_vect)\n",
        "print(accuracy_score(y_test, pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7841210833775889\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-05T02:41:42.736970Z",
          "start_time": "2021-03-05T02:41:33.038705Z"
        },
        "id": "rgf6cQMOqJ_J"
      },
      "source": [
        "# stop words 필터링 추가, ngram을 기본(1,1)에서 (1,2)로 max_df=300으로 변경해 피처 벡터화 적용\n",
        "tfidf_vect = TfidfVectorizer(stop_words='english', ngram_range=(1,2), max_df=300)\n",
        "tfidf_vect.fit(X_train)\n",
        "X_train_tfidf_vect = tfidf_vect.transform(X_train)\n",
        "X_test_tfidf_vect = tfidf_vect.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-05T02:47:39.984613Z",
          "start_time": "2021-03-05T02:41:44.475637Z"
        },
        "id": "40iQTbJiqJ_J",
        "outputId": "f2ddc0e1-3fe5-4abb-e56b-447e3fd53271"
      },
      "source": [
        "# 램 16기가로도 모자른거 실환가?\n",
        "lr_clf = LogisticRegression()\n",
        "lr_clf.fit(X_train_tfidf_vect, y_train)\n",
        "pred = lr_clf.predict(X_test_tfidf_vect)\n",
        "print(accuracy_score(y_test, pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.774429102496017\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mB3pZa45qJ_J"
      },
      "source": [
        "\\[과제]  \n",
        "Random Forest, SVM, DT, KNN을 적용하여 뉴스그룹에 대한 분류 예측을 수행하세요"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2021-03-05T02:49:16.031Z"
        },
        "id": "37aryusSqJ_K",
        "outputId": "0f781c5b-ea3a-41c8-b2fd-1ef1afa8d4a6"
      },
      "source": [
        "# 집에서 해보자...\n",
        "params = {\n",
        "    'C': [5, 10],\n",
        "}\n",
        "gscv_lr = GridSearchCV(LogisticRegression(), param_grid = params, cv=3, scoring='accuracy', verbose=1, n_jobs=2)\n",
        "gscv_lr.fit(X_train_tfidf_vect, y_train)\n",
        "print(gscv_lr.best_params_)\n",
        "pred = gscv_lr.predict(X_test_tfidf_vect)\n",
        "print(accuracy_score(y_test, pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n",
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   6 out of   6 | elapsed: 14.6min finished\n",
            "{'C': 10}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'accuracy_scorec' is not defined",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-11-16f5231c346f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgscv_lr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgscv_lr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_tfidf_vect\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy_scorec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m: name 'accuracy_scorec' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-05T03:22:17.842860Z",
          "start_time": "2021-03-05T03:22:16.141616Z"
        },
        "id": "0KFHHWiOqJ_K",
        "outputId": "c0cd7942-e535-432e-d924-005ce159665f"
      },
      "source": [
        "# 사이킷런 파이프라인 사용 및  LR 결합\n",
        "from sklearn.pipeline import Pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('tfidf_vect', TfidfVectorizer(stop_words='english', ngram_range=(1,2), max_df=300)),\n",
        "    ('lr_clf', LogisticRegression(C=10))\n",
        "])\n",
        "pipeline.fit(X_train,y_train)\n",
        "pred = pipeline.predict(X_test)\n",
        "print(accuracy_score(y_test, pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7980616038236856\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_1FjFqXqJ_K"
      },
      "source": [
        "# 사이킷런 파이프라인과 GridSearchCV와의 결합\n",
        "pipeline = Pipeline([\n",
        "    ('tfidf_vect', TfidfVectorizer(stop_words='english', ngram_range=(1,2), max_df=300)),\n",
        "    ('lr_clf', LogisticRegression(C=10))\n",
        "])\n",
        "params = {'tfidf_vect_ngram_range': [(1,1), (1,2), (1,3)],\n",
        "          'tfidf_vect_max_df': [100, 300, 700],\n",
        "          'lr_clf_C' : [1,5,10]\n",
        "         }\n",
        "grid_cv_pipe = GridSearchCV(pipeline, param_grid = params, cv=3, scoring = 'accuracy', verbose=1, n_jobs=2)\n",
        "grid_cv_pipe.fit(X_train, y_train)\n",
        "print(grid_cv_pipe.best_params-, grid_cv_pipe.best_param.best_score_)\n",
        "pred = grid_cv_pipe.predict(X_train_test)\n",
        "print(accuracy_score(y_test, pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVACAg7nqJ_K"
      },
      "source": [
        "### 감정 분석 - 네이버 영화 평점\n",
        "    네이버 영화 평점 train 데이터 기반으로 다음 사항에 유의하여 감정분석 후 test 데이터를 이용해 최종 감정 예측을 수행하세요.\n",
        "    네이버 영화 평점 데이터는 http://github.com/e9t/nsmc에서 ratings.txt(전체), ratings._train.txt(학습), rating_test.txt(테스트)를 다운로드\n",
        "\n",
        "1. 데이터 세트 요약 정보\n",
        "\n",
        "- 칼럼 : id, document\n",
        "- label : the sentiment class of the review (0:negative, 1: positive)\n",
        "- ratings.txt : All 200K reviews\n",
        "- ratings_test : 50K reviews held out for testing\n",
        "- ratings_train : 150K reviews for training\n",
        "- 정규 표현식을 이용하여 숫자를 공백으로 변경(정규 표현식으로 \\d 는 숫자를 의미함.)\n",
        "\n",
        "2. 과정\n",
        "- 테스트 데이터 셋을 로딩하고 동일하게 Null 및 숫자를 공백으로 변환\n",
        "- 한글 형태소 분석을 통해 형태소 단어로 토큰화(tokenizer 사용자 함수 작성 추천)\n",
        "    - 한글 형태소 엔진은 SNS 분석에 적합한 Okt 클래스를 이용\n",
        "    - morphs() 메서드는 입력 인자로 들어온 문장을 형태소 단어 형태로 토큰화해 list 객체 반환\n",
        "    - Okt 객체의 morphs( ) 객체를 이용한 tokenizer를 사용. ngram_range는 (1,2)\n",
        "- 사이킷런의 TfidVectorizor를 이용, TF-IDF 피처 모델을 생성(10분 이상 소요)\n",
        "- DT, RF, LR 을 이용하여 감성 분석 Classification 수행\n",
        "- 3개 분류 모델별 교차 검증 및 Parameter 최적화를 GridSearchCV 를 이용하여 수행(최적 분류 모델 선정)\n",
        "    - DT params = {'max_depth':[2,3,5,10], 'min_samples_split':[2,3,5],'min_samples_leaf':[1,5,8]}\n",
        "    - RF params = {'n_estimators':[50,100, 200], 'max_depth':[2,3,5], 'min_samples_leaf':[1,5,8]}\n",
        "    - LR params = { 'C': [1 ,3.5, 4.5, 5.5, 10 ] }\\ C는 규제 강도를 조절하는 alpha 값의 역수로 작을 수록 규제 강도가 큼\n",
        "- param_grid=params , cv=3 ,scoring='accuracy', verbose=1(학습진행 상황 표시)\n",
        "- 학습데이터에 사용된 TfidVectorizer 객체 변수인 tfidf_vect를 이용해 transform()을 테스트 데이터의 document 칼럼에 수행"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-09T00:45:14.604570Z",
          "start_time": "2021-03-09T00:45:14.084761Z"
        },
        "id": "jcueiLxpqJ_K",
        "outputId": "081ab0ea-1b9f-4873-d03f-8cefdc17c247"
      },
      "source": [
        "ratings = pd.read_table('../dataset/ratings.txt')\n",
        "ratings_test = pd.read_table('../dataset/ratings_test.txt')\n",
        "ratings_train = pd.read_table('../dataset/ratings_train.txt')\n",
        "ratings_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>document</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9976970</td>\n",
              "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3819312</td>\n",
              "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10265843</td>\n",
              "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9045019</td>\n",
              "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6483659</td>\n",
              "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149995</th>\n",
              "      <td>6222902</td>\n",
              "      <td>인간이 문제지.. 소는 뭔죄인가..</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149996</th>\n",
              "      <td>8549745</td>\n",
              "      <td>평점이 너무 낮아서...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149997</th>\n",
              "      <td>9311800</td>\n",
              "      <td>이게 뭐요? 한국인은 거들먹거리고 필리핀 혼혈은 착하다?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149998</th>\n",
              "      <td>2376369</td>\n",
              "      <td>청춘 영화의 최고봉.방황과 우울했던 날들의 자화상</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149999</th>\n",
              "      <td>9619869</td>\n",
              "      <td>한국 영화 최초로 수간하는 내용이 담긴 영화</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>150000 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "              id                                           document  label\n",
              "0        9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
              "1        3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
              "2       10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
              "3        9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
              "4        6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1\n",
              "...          ...                                                ...    ...\n",
              "149995   6222902                                인간이 문제지.. 소는 뭔죄인가..      0\n",
              "149996   8549745                                      평점이 너무 낮아서...      1\n",
              "149997   9311800                    이게 뭐요? 한국인은 거들먹거리고 필리핀 혼혈은 착하다?      0\n",
              "149998   2376369                        청춘 영화의 최고봉.방황과 우울했던 날들의 자화상      1\n",
              "149999   9619869                           한국 영화 최초로 수간하는 내용이 담긴 영화      0\n",
              "\n",
              "[150000 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-09T00:45:44.134423Z",
          "start_time": "2021-03-09T00:45:43.782704Z"
        },
        "id": "cJWWnhL6qJ_L"
      },
      "source": [
        "ratings.document.replace('\\d', ' ', inplace=True)\n",
        "ratings.fillna(' ', inplace=True)\n",
        "ratings_test.fillna(' ', inplace=True)\n",
        "ratings_train.fillna(' ', inplace=True)\n",
        "ratings_test['document'] = ratings_test['document'].apply(lambda x: re.sub(r\"\\d+\", \" \", x))\n",
        "ratings_train['document'] = ratings_train['document'].apply(lambda x: re.sub(r\"\\d+\", \" \", x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-09T00:47:27.701821Z",
          "start_time": "2021-03-09T00:47:27.660298Z"
        },
        "id": "YOpS9pRLqJ_L"
      },
      "source": [
        "def ko_tokenizer(text):\n",
        "    return okt.morphs(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-09T01:21:02.567989Z",
          "start_time": "2021-03-09T00:48:38.709153Z"
        },
        "id": "mvZfJMzmqJ_L",
        "outputId": "c76c5071-bdf0-47bc-8951-e44c9263048e"
      },
      "source": [
        "tf_idf = TfidfVectorizer(tokenizer=ko_tokenizer, ngram_range=(1,2), min_df=3, max_df=0.9)\n",
        "tf_idf.fit(ratings_train.document)\n",
        "X_train = tf_idf.transform(ratings_train.document)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "C:\\Users\\admin\\anaconda3\\envs\\ml_python\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-09T01:25:59.021263Z",
          "start_time": "2021-03-09T01:25:35.445671Z"
        },
        "id": "pc7crTsaqJ_L",
        "outputId": "0b83e1f6-8176-4090-bedf-f99dc3072ec8"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "lr_clf = LogisticRegression()\n",
        "params = {'C': [1, 3.5, 4.5, 5.5, 10]}\n",
        "gscv = GridSearchCV(lr_clf, param_grid = params, cv=3, scoring='accuracy', verbose=1, n_jobs=4)\n",
        "gscv.fit(X_train, ratings_train.label)\n",
        "print(gscv.best_params_, np.round(gscv.best_score_, 4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=4)]: Done  15 out of  15 | elapsed:   19.4s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'C': 3.5} 0.8592\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-09T01:32:56.052403Z",
          "start_time": "2021-03-09T01:26:17.714088Z"
        },
        "id": "Bn53oSKlqJ_M",
        "outputId": "274c4a9d-e5e8-492a-8265-7bdeca599981"
      },
      "source": [
        "X_test = tf_idf.transform(ratings_test.document)\n",
        "pred = gscv.predict(X_test)\n",
        "print(accuracy_score(pred, ratings_test.label).round(4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8618\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-09T01:34:35.771532Z",
          "start_time": "2021-03-09T01:32:56.271059Z"
        },
        "id": "_TqJ_QVNqJ_M",
        "outputId": "b1fdbb8c-2629-4a98-aacf-ab708c365d73"
      },
      "source": [
        "params =  {'max_depth':[2,3,5,10], 'min_samples_split':[2,3,5],'min_samples_leaf':[1,5,8]}\n",
        "dt_clf = GridSearchCV(DecisionTreeClassifier(), param_grid=params, cv=3, scoring='accuracy', n_jobs=-1)\n",
        "rf_clf = GridSearchCV(DecisionTreeClassifier(), param_grid=params, cv=3, scoring='accuracy', n_jobs=-1)\n",
        "dt_clf.fit(X_train, ratings_train.label)\n",
        "rf_clf.fit(X_train, ratings_train.label)\n",
        "print(\"DecisionTree: \", dt_clf.best_params_, dt_clf.best_score_.round(4))\n",
        "print(\"RandomForest: \", rf_clf.best_params_, rf_clf.best_score_.round(4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DecisionTree:  {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 5} 0.5848\n",
            "RandomForest:  {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 5} 0.5848\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-09T01:41:00.295166Z",
          "start_time": "2021-03-09T01:41:00.241458Z"
        },
        "id": "5YOlnQwYqJ_M",
        "outputId": "335a5699-9184-4f26-b013-2b99c02f31aa"
      },
      "source": [
        "pred = dt_clf.predict(X_test)\n",
        "print(\"DecisionTree:\",accuracy_score(pred, ratings_test.label).round(4))\n",
        "pred = rf_clf.predict(X_test)\n",
        "print(\"RandomForest:\",accuracy_score(pred, ratings_test.label).round(4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DecisionTree: 0.5862\n",
            "RandomForest: 0.5862\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZpJOEapqJ_N"
      },
      "source": [
        "### 토픽 모델링\n",
        "- 머신러닝 기반의 토픽 모델링을 적용하여 문서 집합에 숨어 있는 주제를 찾아냄\n",
        "- 사람이 수행하는 토픽 모델링은 더 함축적인 의미로 문장을 \"요약\"하는 것에 반해 머신 러닝 기반은 중심 단어를 함축적으로 \"추출\"\n",
        "- LSA(Latent Sementic Analysis)와 LDA 기법\n",
        "    - LSA: 단어-문서 행렬, 단어-문맥행렬 등 \n",
        "    - LDA:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-09T01:43:56.363569Z",
          "start_time": "2021-03-09T01:43:51.284849Z"
        },
        "id": "E-nTQ7xqqJ_N",
        "outputId": "a27e9037-299d-44a3-d414-e9fd92cc4b45"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "cats = ['rec.motorcycles', 'rec.sport.baseball', 'comp.graphics', 'comp.windows.x', 'talk.politics.mideast','soc.religion.christian', 'sci.electronics', 'sci.med']\n",
        "new_df = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'), categories= cats)\n",
        "count_vect = CountVectorizer(max_df=0.95, max_features=1000, min_df=2, stop_words='english', ngram_range=(1,2))\n",
        "feat_vect = count_vect.fit_transform(new_df.data)\n",
        "print('CountVectorizer Shape:', feat_vect.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CountVectorizer Shape: (7862, 1000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-09T02:07:29.001915Z",
          "start_time": "2021-03-09T02:07:01.268470Z"
        },
        "id": "kJqyaJpSqJ_N",
        "outputId": "872506aa-9b70-4db1-a2a5-9e5560c36790"
      },
      "source": [
        "lda = LatentDirichletAllocation(n_components=len(cats), random_state=0)\n",
        "lda.fit(feat_vect)\n",
        "print(lda.components_.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(8, 1000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-09T02:13:35.317264Z",
          "start_time": "2021-03-09T02:13:35.273373Z"
        },
        "id": "5IQxGy3KqJ_N",
        "outputId": "9c495e97-4e84-4083-b3c5-8b6bbfead689"
      },
      "source": [
        "def display_topics(model, feature_names, no_top_words):\n",
        "    for index, topic in enumerate(model.components_):\n",
        "        print(\"Topic #\", index)\n",
        "        topic_word_indexes = topic.argsort()[::-1]\n",
        "        topic_indexes = topic_word_indexes[:no_top_words]\n",
        "        feature_concat = '  '.join([feature_names[i] for i in topic_indexes])\n",
        "        print(feature_concat)\n",
        "\n",
        "feature_names = count_vect.get_feature_names()\n",
        "display_topics(lda, feature_names, 10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Topic # 0\n",
            "year  10  game  medical  health  team  12  20  disease  1993\n",
            "Topic # 1\n",
            "don  just  know  like  said  people  time  think  didn  ve\n",
            "Topic # 2\n",
            "image  file  jpeg  program  gif  output  format  images  color  files\n",
            "Topic # 3\n",
            "like  think  don  just  use  does  know  good  time  people\n",
            "Topic # 4\n",
            "armenian  israel  jews  armenians  turkish  people  israeli  jewish  government  war\n",
            "Topic # 5\n",
            "edu  com  available  graphics  ftp  window  motif  data  pub  mail\n",
            "Topic # 6\n",
            "god  jesus  people  church  christ  believe  christian  does  christians  say\n",
            "Topic # 7\n",
            "thanks  dos  use  windows  using  does  help  like  need  display\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bz4Fio8FqJ_O"
      },
      "source": [
        "## 문서 군집화\n",
        "- 비슷한 텍스트 구성의 문서를 군집화하여 같은 카테고리 소속으로 분류\n",
        "- 학습 데이터 세트가 필요없는 비지도 학습 기반으로 동작"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-09T02:41:47.054157Z",
          "start_time": "2021-03-09T02:41:46.745081Z"
        },
        "id": "Fxd74eCQqJ_O",
        "outputId": "118c7206-0b44-4a15-f966-0fb942343290"
      },
      "source": [
        "import glob, os\n",
        "path = '../dataset/topics/'\n",
        "files = glob.glob(os.path.join(path, '*.data'))\n",
        "\n",
        "filename_list = []\n",
        "opinion_text = []\n",
        "\n",
        "for file_ in files:\n",
        "    df = pd.read_table(file_, index_col=None, header=0, encoding='latin1')\n",
        "    filename_list.append(file_.split(\"\\\\\")[-1].split(\".\")[0])\n",
        "    opinion_text.append(df.to_string())\n",
        "document_df = pd.DataFrame({'filename': filename_list, 'opinion_text': opinion_text})\n",
        "document_df.head(4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>opinion_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>voice_garmin_nuvi_255W_gps</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>updates_garmin_nuvi_255W_gps</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>video_ipod_nano_8gb</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>transmission_toyota_camry_2007</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                         filename  \\\n",
              "0      voice_garmin_nuvi_255W_gps   \n",
              "1    updates_garmin_nuvi_255W_gps   \n",
              "2             video_ipod_nano_8gb   \n",
              "3  transmission_toyota_camry_2007   \n",
              "\n",
              "                                        opinion_text  \n",
              "0                                                ...  \n",
              "1                                                ...  \n",
              "2                                                ...  \n",
              "3                                                ...  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-09T02:48:26.268901Z",
          "start_time": "2021-03-09T02:48:25.719215Z"
        },
        "id": "PLegaeE7qJ_O",
        "outputId": "e5b89508-7bea-4239-e2a1-b3ae8f2910ee"
      },
      "source": [
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-09T03:14:37.504939Z",
          "start_time": "2021-03-09T03:14:37.489145Z"
        },
        "id": "Bcnm9KR1qJ_O"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "\n",
        "remove_punct_dict = dict( (ord(punct), None) for punct in string.punctuation )\n",
        "lemma = WordNetLemmatizer()\n",
        "\n",
        "def LemTokens(tokens):\n",
        "    return [lemma.lemmatize(token) for token in tokens]\n",
        "def LeNormalize(text):\n",
        "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-09T03:14:47.905336Z",
          "start_time": "2021-03-09T03:14:45.900898Z"
        },
        "id": "1fcOeqcCqJ_O"
      },
      "source": [
        "# 피처 벡터화\n",
        "tfidf = TfidfVectorizer(tokenizer = LeNormalize,stop_words='english', max_df=0.85, ngram_range=(1,2))\n",
        "feat_vect = tfidf.fit_transform(document_df['opinion_text'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-09T05:33:29.082424Z",
          "start_time": "2021-03-09T05:33:28.881855Z"
        },
        "scrolled": true,
        "id": "GJTQ65rSqJ_P",
        "outputId": "b20500bd-b0dd-43be-8e9c-90f17e9f0064"
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "kmeans =KMeans(max_iter=100000, n_clusters=5)\n",
        "kmeans.fit(feat_vect)\n",
        "cluster_label = kmeans.labels_\n",
        "cluster_centers = kmeans.cluster_centers_\n",
        "document_df['label'] = cluster_label\n",
        "for i in sorted(document_df.label.unique()):\n",
        "    print('=============================label {}============================='.format(i))\n",
        "    print( document_df.loc[document_df.label==i, \"filename\"] )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=============================label 0=============================\n",
            "4             staff_swissotel_chicago\n",
            "5         staff_bestwestern_hotel_sfo\n",
            "10    service_swissotel_hotel_chicago\n",
            "11         service_holiday_inn_london\n",
            "12      service_bestwestern_hotel_sfo\n",
            "18            rooms_swissotel_chicago\n",
            "19        rooms_bestwestern_hotel_sfo\n",
            "20            room_holiday_inn_london\n",
            "22           price_holiday_inn_london\n",
            "26      parking_bestwestern_hotel_sfo\n",
            "29        location_holiday_inn_london\n",
            "30     location_bestwestern_hotel_sfo\n",
            "35         free_bestwestern_hotel_sfo\n",
            "36             food_swissotel_chicago\n",
            "37            food_holiday_inn_london\n",
            "49     bathroom_bestwestern_hotel_sfo\n",
            "Name: filename, dtype: object\n",
            "=============================label 1=============================\n",
            "0          voice_garmin_nuvi_255W_gps\n",
            "1        updates_garmin_nuvi_255W_gps\n",
            "6                      speed_windows7\n",
            "7          speed_garmin_nuvi_255W_gps\n",
            "16        screen_garmin_nuvi_255W_gps\n",
            "17     satellite_garmin_nuvi_255W_gps\n",
            "39                  features_windows7\n",
            "41       display_garmin_nuvi_255W_gps\n",
            "42    directions_garmin_nuvi_255W_gps\n",
            "50      accuracy_garmin_nuvi_255W_gps\n",
            "Name: filename, dtype: object\n",
            "=============================label 2=============================\n",
            "2             video_ipod_nano_8gb\n",
            "8             sound_ipod_nano_8gb\n",
            "9        size_asus_netbook_1005ha\n",
            "14          screen_netbook_1005ha\n",
            "15           screen_ipod_nano_8gb\n",
            "24     performance_netbook_1005ha\n",
            "31        keyboard_netbook_1005ha\n",
            "46    battery-life_netbook_1005ha\n",
            "47     battery-life_ipod_nano_8gb\n",
            "48     battery-life_amazon_kindle\n",
            "Name: filename, dtype: object\n",
            "=============================label 3=============================\n",
            "3     transmission_toyota_camry_2007\n",
            "13           seats_honda_accord_2008\n",
            "21         quality_toyota_camry_2007\n",
            "25     performance_honda_accord_2008\n",
            "28         mileage_honda_accord_2008\n",
            "32        interior_toyota_camry_2007\n",
            "33        interior_honda_accord_2008\n",
            "34     gas_mileage_toyota_camry_2007\n",
            "43         comfort_toyota_camry_2007\n",
            "44         comfort_honda_accord_2008\n",
            "Name: filename, dtype: object\n",
            "=============================label 4=============================\n",
            "23              price_amazon_kindle\n",
            "27         navigation_amazon_kindle\n",
            "38              fonts_amazon_kindle\n",
            "40    eyesight-issues_amazon_kindle\n",
            "45            buttons_amazon_kindle\n",
            "Name: filename, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-09T05:42:54.104271Z",
          "start_time": "2021-03-09T05:42:54.057480Z"
        },
        "id": "Ndaq4gUAqJ_P",
        "outputId": "e4bbe6bb-d940-4747-b7ac-ad8fe9e568cc"
      },
      "source": [
        "# 군집별 top n개의 핵심단어, 그 단어의 중심 위치 상대값, 대상 파일명들을 반환\n",
        "def get_cluster_details(cluster_model, cluster_data, feature_names, cluster_num, top_n_features=10):\n",
        "    cluster_details={}\n",
        "    # 각 클러스터 레이블별 feature들의 center 값들 내림차순으로 정렬 후의 인덱스를 반환\n",
        "    centroid_feature_ordered_ind = cluster_model.cluster_centers_.argsort()[:, ::-1]\n",
        "    \n",
        "    #개별 클러스터 레이블 별로\n",
        "    for cluster_num in range(cluster_num):\n",
        "        # 개별 군집별 정보를 empty dict 할당\n",
        "        cluster_details[cluster_num] = {}\n",
        "        cluster_details[cluster_num]['cluster'] = cluster_num\n",
        "        # top n개의 피처 단어를  구하기\n",
        "        top_n_feature_indexes = centroid_feature_ordered_ind[cluster_num, :top_n_features]\n",
        "        top_features = [feature_names[ind] for ind in top_n_feature_indexes ]\n",
        "        \n",
        "        #top_feature_indexes를 이용해 해당 피쳐 단어와 중심 위치 상대값 구하기\n",
        "        top_features_values = cluster_model.cluster_centers_[cluster_num, top_n_feature_indexes].tolist()\n",
        "        \n",
        "        # cluster_details 딕셔너리 객체에 개별 군집별 핵심 단어와 중심위치 상대값 파일명 업데이트\n",
        "        cluster_details[cluster_num]['top_features'] = top_features\n",
        "        cluster_details[cluster_num]['top_features_value'] = top_features_values\n",
        "        filenames = cluster_data[cluster_data['label']==cluster_num]['filename']\n",
        "        filenames = filenames.tolist()\n",
        "        cluster_details[cluster_num]['filename'] = filenames\n",
        "        \n",
        "    return cluster_details\n",
        "\n",
        "def print_cluster_details(cluster_details):\n",
        "    for cluster_num, cluster_detail in cluster_details.items():\n",
        "        print('### Cluster {0}'.format(cluster_num))\n",
        "        print('Top features:', cluster_detail['top_features'])\n",
        "        print('Review 파일명:', cluster_detail['filename'][:7])\n",
        "feature_names =tfidf.get_feature_names()\n",
        "cluster_details =  get_cluster_details(kmeans, document_df, feature_names, 5)\n",
        "print_cluster_details(cluster_details)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "### Cluster 0\n",
            "Top features: ['room', 'hotel', 'service', 'staff', 'location', 'food', 'bathroom', 'clean', 'price', 'parking']\n",
            "Review 파일명: ['staff_swissotel_chicago', 'staff_bestwestern_hotel_sfo', 'service_swissotel_hotel_chicago', 'service_holiday_inn_london', 'service_bestwestern_hotel_sfo', 'rooms_swissotel_chicago', 'rooms_bestwestern_hotel_sfo']\n",
            "### Cluster 1\n",
            "Top features: ['direction', 'voice', 'map', 'screen', 'speed limit', 'speed', 'accurate', 'satellite', 'limit', 'feature']\n",
            "Review 파일명: ['voice_garmin_nuvi_255W_gps', 'updates_garmin_nuvi_255W_gps', 'speed_windows7', 'speed_garmin_nuvi_255W_gps', 'screen_garmin_nuvi_255W_gps', 'satellite_garmin_nuvi_255W_gps', 'features_windows7']\n",
            "### Cluster 2\n",
            "Top features: ['battery', 'screen', 'battery life', 'life', 'keyboard', 'video', 'size', 'sound', 'performance', 'ipod']\n",
            "Review 파일명: ['video_ipod_nano_8gb', 'sound_ipod_nano_8gb', 'size_asus_netbook_1005ha', 'screen_netbook_1005ha', 'screen_ipod_nano_8gb', 'performance_netbook_1005ha', 'keyboard_netbook_1005ha']\n",
            "### Cluster 3\n",
            "Top features: ['interior', 'seat', 'mileage', 'gas', 'comfortable', 'gas mileage', 'transmission', 'car', 'performance', 'quality']\n",
            "Review 파일명: ['transmission_toyota_camry_2007', 'seats_honda_accord_2008', 'quality_toyota_camry_2007', 'performance_honda_accord_2008', 'mileage_honda_accord_2008', 'interior_toyota_camry_2007', 'interior_honda_accord_2008']\n",
            "### Cluster 4\n",
            "Top features: ['kindle', 'page', 'button', 'font', 'book', 'eye', 'price', 'navigation', 'font size', 'easy']\n",
            "Review 파일명: ['price_amazon_kindle', 'navigation_amazon_kindle', 'fonts_amazon_kindle', 'eyesight-issues_amazon_kindle', 'buttons_amazon_kindle']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}