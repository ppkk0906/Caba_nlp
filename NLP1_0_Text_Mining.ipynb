{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP1-0 Text Mining.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO/Ijj604AyavWuI+t1AQIW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ppkk0906/Caba_nlp/blob/main/NLP1_0_Text_Mining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfgFhH-17V_W"
      },
      "source": [
        "## NLP, 텍스트 분석\r\n",
        "- Natural Language Processing: 자연어처리\r\n",
        "- 텍스트 분석: 비정형 텍스트에서 의미있는 정보 추출\r\n",
        "- NLP와 텍스트 분석의 근간에는 머신러닝이 존재. 과거 언어적인 룰 기반 시스템에서 텍스트 데이터 기반으로 모델을 학습하고 예측\r\n",
        "- 텍스트 분석은 머신러닝, 언어 이해, 통계 등을 활용한 모델 수립, 정보 추출을 통해 인사이트 및 예측 분석 등의 분석 작업 수행\r\n",
        "  - 텍스트 분류: 신문기사 카테고리 분류, 스팸 메일 검출, 지도학습\r\n",
        "  - 감성 분석: 감정/판단/믿음/의견/기분 등의 주관적 요소 분석, 지도 또는 비지도 학습\r\n",
        "  - 텍스트 요약: 텍스트 내에서 중요한 주제나 중심 사상을 추출. 토픽 모델링\r\n",
        "  - 텍스트 군집화와 유사도 측정: 비슷한 유형의 문서에 대해 군집화 수행, 비지도 학습\r\n",
        "\r\n",
        "## 텍스트 분석 수행 프로세스\r\n",
        "  - 텍스트 정규화\r\n",
        "    - 클랜징, 토큰화, 필터링/스톱워드 제거/철자 수정\r\n",
        "  - 피처 벡터화 변환\r\n",
        "    - Bag of words: Count 기반, TF-IDF 기반\r\n",
        "    - Word2Vec\r\n",
        "  - ML모델 수립 및 학습/예측/평가\r\n",
        "## 텍스트 전처리 - 텍스트 정규화\r\n",
        "  - 클렌징: 분석에 방해되는 불필요한 문자, 기호를 사전에 제거 예) html,xml 태그\r\n",
        "  - 토큰화: 문서에 문장을 분리하는 문장 토큰화와 단어를 토큰으로 분리하는 단어 토큰화\r\n",
        "  - 필터링/스톱워드 제거/철자 수정: 분석에 큰 의미가 없는 단어를 제거\r\n",
        "  - Stemming Lemmatization: 문법적, 의미적으로 변화하는 단어의 웥ㄴ형을 찾음\r\n",
        "    - Stemming: 원형 단어로 변환 시 일반적인 방법을 적용하거나 더 단순화된 방법을 찾음\r\n",
        "    - Lemmatization: Stemming보다 정교하며 의미론적인 기반에서 단어의 원형을 찾음\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMQqIpOu7GUZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ea94c9f-ce75-40d0-d053-b9b8edd4b3c5"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WR7fmJvJ7VQo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c974cd0-7c5c-4577-c83d-cc9ddb9bda33"
      },
      "source": [
        "from nltk import sent_tokenize\r\n",
        "text_sample = 'The Matrix is everywhere ites all around us, here even in this room. You can see it out your window or on your television. You feel it when you go to work, or go to church or pay your taxes'\r\n",
        "sentences = sent_tokenize(text_sample)\r\n",
        "print(sentences)\r\n",
        "print(type(sentences), len(sentences))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['The Matrix is everywhere ites all around us, here even in this room.', 'You can see it out your window or on your television.', 'You feel it when you go to work, or go to church or pay your taxes']\n",
            "<class 'list'> 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8N-Gzf8EQO-",
        "outputId": "aad29924-4e09-4d03-fae4-fbc6de27b42c"
      },
      "source": [
        "# 단어 토큰화(work_tokenize): 공백, 콤마, 마침표, 개행문자, 정규표현식\r\n",
        "from nltk import word_tokenize\r\n",
        "words = word_tokenize(sentences[0])\r\n",
        "print(words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['The', 'Matrix', 'is', 'everywhere', 'ites', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-X_IdfSFGxa",
        "outputId": "0b2bbb49-37e2-4e41-9411-f8b4d84475c8"
      },
      "source": [
        "def get_words(text):\r\n",
        "  sentences = sent_tokenize(text)\r\n",
        "  words=[]\r\n",
        "  for s in sentences:\r\n",
        "    for i in word_tokenize(s):\r\n",
        "      words.append(i.lower())\r\n",
        "  return words\r\n",
        "\r\n",
        "print(get_words(text_sample))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['the', 'matrix', 'is', 'everywhere', 'ites', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.', 'you', 'can', 'see', 'it', 'out', 'your', 'window', 'or', 'on', 'your', 'television', '.', 'you', 'feel', 'it', 'when', 'you', 'go', 'to', 'work', ',', 'or', 'go', 'to', 'church', 'or', 'pay', 'your', 'taxes']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "866sGutRHmCm",
        "outputId": "499fde2c-f8a0-4631-9a63-398bd73f00fb"
      },
      "source": [
        "# 스톱워드 제거: the, a, is, in, will과 같이 문맥적으로 큰 의미가 없는 단어 제거\r\n",
        "import nltk\r\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPPU1Q7MHBCo",
        "outputId": "ae2e3b60-a067-442a-d9b8-c2bf74a942d3"
      },
      "source": [
        "# NLTK english stopwords 확인\r\n",
        "print(len(nltk.corpus.stopwords.words('english')))\r\n",
        "print(nltk.corpus.stopwords.words('english'[:20]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "179\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgOZWhrRMzeP",
        "outputId": "04e50915-0e86-400a-e0ad-bf691b9e24f9"
      },
      "source": [
        "# 문법적 또는 의미적으로 변화하는 단어의 원형을 찾는 방법\r\n",
        "# Stemmer(LancasterStemmer)\r\n",
        "from nltk.stem import LancasterStemmer\r\n",
        "stemmer = LancasterStemmer()\r\n",
        "print(stemmer.stem('working'),stemmer.stem('works'),stemmer.stem('worked'))\r\n",
        "print(stemmer.stem('amusing'),stemmer.stem('aumses'),stemmer.stem('amused'))\r\n",
        "print(stemmer.stem('fancier'),stemmer.stem('fancist'))\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "work work work\n",
            "amus aums amus\n",
            "fant fant\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfofLufglcZP",
        "outputId": "3fd4a7ec-07e3-4175-cb3f-213e6eb69307"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('wordnet')\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJe5XMgwqKHJ",
        "outputId": "f5e3449e-996b-437e-c1a5-b90c2d2459ab"
      },
      "source": [
        "# Lemmatization(WordNetLemmatizer) : 정확한 원형 단어 추출을 위해 단어의 품사를 직접 입력\r\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\r\n",
        "\r\n",
        "lemma = WordNetLemmatizer()\r\n",
        "print(lemma.lemmatize('working','v'),lemma.lemmatize('works','v'),lemma.lemmatize('worked','v'))\r\n",
        "print(lemma.lemmatize('amusing','v'),lemma.lemmatize('aumses','v'),lemma.lemmatize('amused','v'))\r\n",
        "print(lemma.lemmatize('fancier','a'),lemma.lemmatize('fancier','a'))\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "work work work\n",
            "amuse aumses amuse\n",
            "fancy fancy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6BAW0LDsFlR"
      },
      "source": [
        "## 12345"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCAg4BIvyU7g",
        "outputId": "fc981d0f-5ff8-4407-f8de-1b5266490380"
      },
      "source": [
        "#ndarray 객체 생성\r\n",
        "import numpy as np\r\n",
        "dense = np.array([[3,0,1], [0,2,0]])\r\n",
        "dense"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3, 0, 1],\n",
              "       [0, 2, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9zgxT-ayhjU",
        "outputId": "e2e715ea-912e-4e7b-8e00-0b24e61a1c71"
      },
      "source": [
        "# 희소행렬 - coo형식\r\n",
        "from scipy import sparse\r\n",
        "data = np.array([3,1,2])\r\n",
        "row_pos = np.array([0,0,1])\r\n",
        "col_pos = np.array([0,2,1])\r\n",
        "sparse_coo = sparse.coo_matrix((data,(row_pos, col_pos)))\r\n",
        "print(sparse_coo)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (0, 0)\t3\n",
            "  (0, 2)\t1\n",
            "  (1, 1)\t2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVX0gkFRzdmV",
        "outputId": "801562e3-f67f-4892-ddba-f4f3c0384d4f"
      },
      "source": [
        "# 희소 행렬 - COO 형식\r\n",
        "dense2 = np.array([[0,0,1,0,0,5],\r\n",
        "                   [1,4,0,3,2,5],\r\n",
        "                   [0,6,0,3,0,0],\r\n",
        "                   [2,0,0,0,0,0],\r\n",
        "                   [0,0,0,7,0,8],\r\n",
        "                   [1,0,0,0,0,0]])\r\n",
        "data2 = np.array([1,5,1,4,3,2,5,6,3,2,7,8,1])\r\n",
        "row_pos2=np.array([0,0,1,1,1,1,1,2,2,3,4,4,5])\r\n",
        "col_pos2=np.array([2,5,0,1,3,4,5,1,3,0,3,5,0])\r\n",
        "print(sparse.coo_matrix((data2,(row_pos2, col_pos2))).toarray(), '\\n')\r\n",
        "#희소 행렬 - CSR 형식\r\n",
        "row_pos_cul = ([0,2,7,9,10,12,13])\r\n",
        "print(sparse.csr_matrix((data2,col_pos2,row_pos_cul)).toarray())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 0 1 0 0 5]\n",
            " [1 4 0 3 2 5]\n",
            " [0 6 0 3 0 0]\n",
            " [2 0 0 0 0 0]\n",
            " [0 0 0 7 0 8]\n",
            " [1 0 0 0 0 0]] \n",
            "\n",
            "[[0 0 1 0 0 5]\n",
            " [1 4 0 3 2 5]\n",
            " [0 6 0 3 0 0]\n",
            " [2 0 0 0 0 0]\n",
            " [0 0 0 7 0 8]\n",
            " [1 0 0 0 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prcK6jQ33NCk",
        "outputId": "23890e2e-fa9d-4d96-dac6-f598c00bf071"
      },
      "source": [
        "# DictVectorizer : 문서에서 단어의 사용빈도를 나타내는 딕셔너리 정보를 입력받아 BOW 인코딩한 수치 벡터로 전환\r\n",
        "from sklearn.feature_extraction import DictVectorizer\r\n",
        "v = DictVectorizer(sparse=False)\r\n",
        "D = [{'A':1, 'B':2}, {'B':2, 'C':1}]\r\n",
        "X = v.fit_transform(D)\r\n",
        "print(X)\r\n",
        "print(v.feature_names_)\r\n",
        "print(v.vocabulary_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1. 2. 0.]\n",
            " [0. 2. 1.]]\n",
            "['A', 'B', 'C']\n",
            "{'A': 0, 'B': 1, 'C': 2}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HV7yGz2W6zad",
        "outputId": "f3a4690c-a565-4c21-abdb-0d13a8205540"
      },
      "source": [
        "# CountVectorizer\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "corpus = [\r\n",
        "          'This is the first document',\r\n",
        "          'This is the second document.',\r\n",
        "          'And the third one...',\r\n",
        "          'Is this the first document?',\r\n",
        "          'The last document???'\r\n",
        "]\r\n",
        "c = CountVectorizer()\r\n",
        "c.fit(corpus)\r\n",
        "print(c.get_feature_names())\r\n",
        "print(c.transform(corpus))\r\n",
        "print(c.vocabulary_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['and', 'document', 'first', 'is', 'last', 'one', 'second', 'the', 'third', 'this']\n",
            "  (0, 1)\t1\n",
            "  (0, 2)\t1\n",
            "  (0, 3)\t1\n",
            "  (0, 7)\t1\n",
            "  (0, 9)\t1\n",
            "  (1, 1)\t1\n",
            "  (1, 3)\t1\n",
            "  (1, 6)\t1\n",
            "  (1, 7)\t1\n",
            "  (1, 9)\t1\n",
            "  (2, 0)\t1\n",
            "  (2, 5)\t1\n",
            "  (2, 7)\t1\n",
            "  (2, 8)\t1\n",
            "  (3, 1)\t1\n",
            "  (3, 2)\t1\n",
            "  (3, 3)\t1\n",
            "  (3, 7)\t1\n",
            "  (3, 9)\t1\n",
            "  (4, 1)\t1\n",
            "  (4, 4)\t1\n",
            "  (4, 7)\t1\n",
            "{'this': 9, 'is': 3, 'the': 7, 'first': 2, 'document': 1, 'second': 6, 'and': 0, 'third': 8, 'one': 5, 'last': 4}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCEOxWotAQJn",
        "outputId": "958a80c3-bcac-4ee8-8441-a204569619f4"
      },
      "source": [
        "print(c.transform(['this is the second document']).toarray())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 1 0 1 0 0 1 1 0 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6rpz4YuBNd-",
        "outputId": "f5808e65-9d5e-4430-dd1c-c58e27c83563"
      },
      "source": [
        "# Stop words는 문서에서 단어장을 생성할  때 무시해도 되는 단어\r\n",
        "# 보통 영어의 관사, 접속사, 한국어의 조사 등\r\n",
        "vect = CountVectorizer(stop_words='english').fit(corpus)\r\n",
        "print(vect.vocabulary_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'document': 0, 'second': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSHjS2RTC6HX"
      },
      "source": [
        "# analyzer, tokenizer, token_pattern 등의 인수로 사용할 토큰 생성기를 선택\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ww0ft3OfDuHI",
        "outputId": "2661cdf1-6a7c-4005-a41f-cf21db80d739"
      },
      "source": [
        "# n-그램은 단어장 생성에 사용할 토큰의 크기 결정\r\n",
        "vect = CountVectorizer(ngram_range=(1,2)).fit(corpus)\r\n",
        "print(vect.vocabulary_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'this': 20, 'is': 5, 'the': 13, 'first': 3, 'document': 2, 'this is': 21, 'is the': 6, 'the first': 14, 'first document': 4, 'second': 11, 'the second': 16, 'second document': 12, 'and': 0, 'third': 18, 'one': 10, 'and the': 1, 'the third': 17, 'third one': 19, 'is this': 7, 'this the': 22, 'last': 8, 'the last': 15, 'last document': 9}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tT_8uQZ2E2gQ"
      },
      "source": [
        "## TF-IDF\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOydLWmOE15H"
      },
      "source": [
        "corpus = [\"\"\"\r\n",
        "The Corpus of Contemporary American English (COCA) is the only large, genre-balanced corpus of American English.\r\n",
        "COCA is probably the most widely-used corpus of English, and it is related to many other corpora of English that we have created, which offer unparalleled insight into variation in English.\r\n",
        "The corpus contains more than one billion words of text (25+ million words each year 1990-2019) from eight genres: spoken, fiction, popular magazines, newspapers, academic texts, and (with the update in March 2020): TV and Movies subtitles, blogs, and other web pages.\r\n",
        "Click on any of the links in the search form to the left for context-sensitive help, and to see the range of queries that the corpus offers.\r\n",
        "\"\"\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ik6uUlhbGsn6",
        "outputId": "1eefc5a5-b8cd-4b9b-9cf6-e5a94aaf4ed3"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "tfidv = TfidfVectorizer(stop_words = 'english').fit(corpus)\r\n",
        "print(tfidv.get_feature_names())\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['1990', '2019', '2020', '25', 'academic', 'american', 'balanced', 'billion', 'blogs', 'click', 'coca', 'contains', 'contemporary', 'context', 'corpora', 'corpus', 'created', 'english', 'fiction', 'form', 'genre', 'genres', 'help', 'insight', 'large', 'left', 'links', 'magazines', 'march', 'million', 'movies', 'newspapers', 'offer', 'offers', 'pages', 'popular', 'probably', 'queries', 'range', 'related', 'search', 'sensitive', 'spoken', 'subtitles', 'text', 'texts', 'tv', 'unparalleled', 'update', 'used', 'variation', 'web', 'widely', 'words', 'year']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gpu6nHsD1veL"
      },
      "source": [
        "[KoNLP 패키지 사용 방법](https://mr-doosun.tistory.com/22)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksQnfNNcv2qF"
      },
      "source": [
        "import jpype\r\n",
        "from konlpy.tag import Okt, Hannanum, Kkma, Komoran, Mecab\r\n"
      ],
      "execution_count": 18,
      "outputs": []
    }
  ]
}